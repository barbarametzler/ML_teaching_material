{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks in action\n",
    "\n",
    "In this tutorial we'll be looking at the building blocks of neural networks and how to implement them in Python with the ```pytorch``` library.\n",
    "\n",
    "\n",
    "### Content:\n",
    "- MNIST Dataset \n",
    "\n",
    "- Supervised neural networks\n",
    "    - Neural networks:\n",
    "        - Single-Layer Feed-Forward NN\n",
    "        - Multi-Layer Feed-Forward NN\n",
    "- Unsupervised neural networks\n",
    "    - Autoencoder\n",
    "    \n",
    "- Convolutional neural network (sneak peek)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "In this practical, we will work with the hand-written digit classification dataset, [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "![](img/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST (\"Modified National Institute of Standards and Technology\") is probably one of the most commonly used datsets in computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. \n",
    "\n",
    "The dataset consists of 10,000 labelled 28 x 28 pixel images of handwritten digets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed-Forward Neural Networks\n",
    "are artificial neural network wherein connections between the nodes do not form any loops or cycles and -as the name says- feed the information forward.\n",
    "\n",
    "![](img/nn.png)\n",
    "\n",
    "#### Single-Layer Feed-Forward NN\n",
    "The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. \n",
    "\n",
    "#### Multi-Layer Feed-Forward NN\n",
    "A multi-layer neural network contains more than one layer of artificial neurons or nodes. They differ widely in design. It is important to note that while single-layer neural networks were useful early in the evolution of AI, the vast majority of networks used today have a multi-layer model. Multi-layer neural networks can be set up in numerous ways. Typically, they have at least one input layer, which sends weighted inputs to a series of hidden layers, and an output layer at the end.\n",
    "\n",
    "#### Autoencoder\n",
    "Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of representation learning. Specifically, we'll design a neural network architecture such that we impose a bottleneck in the network which forces a compressed knowledge representation of the original input. If the input features were each independent of one another, this compression and subsequent reconstruction would be a very difficult task. However, if some sort of structure exists in the data (ie. correlations between input features), this structure can be learned and consequently leveraged when forcing the input through the network's bottleneck.\n",
    "\n",
    "![](img/autoen.png)\n",
    "\n",
    "The picture shows the destinct architecture of an autoencoder that consists of a encoder (left next to bottlenecck) and decoder network (right).\n",
    "\n",
    "#### Convolutional Neural Network\n",
    "You'll learn more about CNNs next week, but this tutorial includes a small intro into their architecture. CNN have special layers - convolutional layers - that are great at picking up on patterns in the input image, such as lines, gradients, circles, or even eyes and faces. It is this property that makes CNNs so powerful for computer vision. \n",
    "\n",
    "\n",
    "\n",
    "###  Building blocks ('layers') of neural networks\n",
    "\n",
    "**Input or Visible Layers**\n",
    "The bottom layer that takes input from your dataset is called the visible layer, because it is the exposed part of the network. Often a neural network is drawn with a visible layer with one neuron per input value or column in your dataset. These are not neurons as described above, but simply pass the input value though to the next layer.\n",
    "\n",
    "**Hidden Layers**\n",
    "Layers after the input layer are called hidden layers because that are not directly exposed to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value.\n",
    "\n",
    "Given increases in computing power and efficient libraries, very deep neural networks can be constructed. Deep learning can refer to having many hidden layers in your neural network. They are deep because they would have been unimaginably slow to train historically, but may take seconds or minutes to train using modern techniques and hardware.\n",
    "\n",
    "**Output Layer**\n",
    "The final hidden layer is called the output layer and it is responsible for outputting a value or vector of values that correspond to the format required for the problem.\n",
    "\n",
    "The choice of activation function in the output layer is strongly constrained by the type of problem that you are modeling. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two mainstream neural network libraries are TensorFlow by Google and PyTorch by Facebook. We will use PyTorch for this practical. Please install the package first, following the instruction. In most cases, you simply need to run the following command on your computer:\n",
    "\n",
    "```pip3 install torch torchvision```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import struct\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since most of you use laptops, we use CPU for training.\n",
    "device = 'cpu'\n",
    "# If some of you would like to try GPU. You can set the device to be CUDA.\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataset we will be using a function by the ```torchvision``` library. It automatically downloads the dataset and saves it in the 'data' folder. We further transfer the images into tensors. A ```torch.Tensor``` is a multi-dimensional matrix containing elements of a single data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False) ## you can set this to False, if you already downloaded it\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing optimizer and criterion\n",
    "\n",
    "For this tutorial we'll be using a little upgrade to the 'basic' stochastic gradient descent (SGD), the **Adam** optimizer. While stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iteratively based in training data. The Adam optimizer combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. It is relatively easy to configure while the default configuration parameters do well on most problems.\n",
    "\n",
    "In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative.\n",
    "\n",
    "Criterion: Because this is a multi-class classification problem we'll be using the ```CrossEntropyLoss```. \n",
    "\n",
    "More information on choosing a suitable loss function: https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "\n",
    "\n",
    "##### Choice of hyper-parameters\n",
    "\n",
    "- **input size:** This should be the size of the MNIST image (28 * 28)\n",
    "- **number of classes:** There's 10 different classes in the MNIST dataset\n",
    "- **number of epochs:** An epoch can is one cycle through the entire training dataset. The number of epoch decides the number of times the weights in the neural network will get updated. The model training should occur on an optimal number of epochs to increase its generalization capacity. There is no fixed number of epochs that will improve your model performance. The number of epochs is actually not that important in comparison to the training and validation loss (i.e. the error). As long as these two losses continue to decrease, the training should continue. In this tutorial we'll keep the number of epochs low for time constraints.\n",
    "- **batch size:** The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In general: Larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster. It's definitely problem and hardware (CPU or GPU) dependent.\n",
    "- **learning rate:** The amount that the weights are updated during training is referred to as the step size or the “learning rate.” Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. 0.1 is commonly used as default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's visualise one of the examples of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2dfc27fa90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the label for the image\n",
    "labels = labels.numpy()\n",
    "lab = np.squeeze(labels[0])\n",
    "lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at different network architectures and how you can build them yourself. As seen in the last tutorial, networks consist of multiple layers. The simplest layer is probably a linear layer - which is basically a simple logistic regression. A linear layer is defined as ```nn.Linear``` in pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Layer Feed-Forward NN\n",
    "The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. \n",
    "\n",
    "Here we are using a ```nn.Linear``` layer which corresponds to a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic model\n",
    "lin_model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.3014\n",
      "Epoch [1/5], Step [200/600], Loss: 2.3090\n",
      "Epoch [1/5], Step [300/600], Loss: 2.3140\n",
      "Epoch [1/5], Step [400/600], Loss: 2.2966\n",
      "Epoch [1/5], Step [500/600], Loss: 2.2859\n",
      "Epoch [1/5], Step [600/600], Loss: 2.3105\n",
      "Epoch [2/5], Step [100/600], Loss: 2.3123\n",
      "Epoch [2/5], Step [200/600], Loss: 2.3440\n",
      "Epoch [2/5], Step [300/600], Loss: 2.2905\n",
      "Epoch [2/5], Step [400/600], Loss: 2.3081\n",
      "Epoch [2/5], Step [500/600], Loss: 2.3128\n",
      "Epoch [2/5], Step [600/600], Loss: 2.3100\n",
      "Epoch [3/5], Step [100/600], Loss: 2.2914\n",
      "Epoch [3/5], Step [200/600], Loss: 2.3184\n",
      "Epoch [3/5], Step [300/600], Loss: 2.3122\n",
      "Epoch [3/5], Step [400/600], Loss: 2.3553\n",
      "Epoch [3/5], Step [500/600], Loss: 2.2841\n",
      "Epoch [3/5], Step [600/600], Loss: 2.2979\n",
      "Epoch [4/5], Step [100/600], Loss: 2.3127\n",
      "Epoch [4/5], Step [200/600], Loss: 2.3464\n",
      "Epoch [4/5], Step [300/600], Loss: 2.3111\n",
      "Epoch [4/5], Step [400/600], Loss: 2.3353\n",
      "Epoch [4/5], Step [500/600], Loss: 2.3148\n",
      "Epoch [4/5], Step [600/600], Loss: 2.3313\n",
      "Epoch [5/5], Step [100/600], Loss: 2.3152\n",
      "Epoch [5/5], Step [200/600], Loss: 2.3115\n",
      "Epoch [5/5], Step [300/600], Loss: 2.3147\n",
      "Epoch [5/5], Step [400/600], Loss: 2.2990\n",
      "Epoch [5/5], Step [500/600], Loss: 2.3173\n",
      "Epoch [5/5], Step [600/600], Loss: 2.3373\n",
      "Accuracy of the model on the 10000 test images: 11 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lin_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size)\n",
    "        outputs = lin_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(lin_model.state_dict(), 'linear_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11% accuracy - We can do better than that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model (after saving it)\n",
    "#lin_model.load_state_dict(torch.load('linear_model.ckpt'))\n",
    "#lin_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Feed-Forward NN\n",
    "\n",
    "An multi-layer network consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. Its multiple layers and non-linear activation distinguish multi-layer network from a linear perceptron. \n",
    "\n",
    "\n",
    "Choice of number of hidden layers: Unfortunately, there is no way to select automatically the number of layers and neurons in each layer. One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. One hidden layer is sufficient for the large majority of problems.\n",
    "\n",
    "```hidden_size``` is the number of nodes in one hidden layer. There is a rule of thumb on how to choose the number of nodes in the layer: The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer. This hyperparameter can be easily tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU() # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x): # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model_nn = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_nn.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.5470\n",
      "Epoch [1/5], Step [200/600], Loss: 0.7052\n",
      "Epoch [1/5], Step [300/600], Loss: 0.7047\n",
      "Epoch [1/5], Step [400/600], Loss: 0.8609\n",
      "Epoch [1/5], Step [500/600], Loss: 0.5021\n",
      "Epoch [1/5], Step [600/600], Loss: 0.5029\n",
      "Epoch [2/5], Step [100/600], Loss: 0.4569\n",
      "Epoch [2/5], Step [200/600], Loss: 0.3818\n",
      "Epoch [2/5], Step [300/600], Loss: 0.4070\n",
      "Epoch [2/5], Step [400/600], Loss: 0.4093\n",
      "Epoch [2/5], Step [500/600], Loss: 0.5039\n",
      "Epoch [2/5], Step [600/600], Loss: 0.5905\n",
      "Epoch [3/5], Step [100/600], Loss: 0.3261\n",
      "Epoch [3/5], Step [200/600], Loss: 0.5587\n",
      "Epoch [3/5], Step [300/600], Loss: 0.6692\n",
      "Epoch [3/5], Step [400/600], Loss: 0.6935\n",
      "Epoch [3/5], Step [500/600], Loss: 0.4774\n",
      "Epoch [3/5], Step [600/600], Loss: 0.5603\n",
      "Epoch [4/5], Step [100/600], Loss: 0.9351\n",
      "Epoch [4/5], Step [200/600], Loss: 0.4546\n",
      "Epoch [4/5], Step [300/600], Loss: 0.9326\n",
      "Epoch [4/5], Step [400/600], Loss: 0.3520\n",
      "Epoch [4/5], Step [500/600], Loss: 0.5906\n",
      "Epoch [4/5], Step [600/600], Loss: 2.3497\n",
      "Epoch [5/5], Step [100/600], Loss: 0.4517\n",
      "Epoch [5/5], Step [200/600], Loss: 0.5518\n",
      "Epoch [5/5], Step [300/600], Loss: 0.6619\n",
      "Epoch [5/5], Step [400/600], Loss: 0.7634\n",
      "Epoch [5/5], Step [500/600], Loss: 0.8070\n",
      "Epoch [5/5], Step [600/600], Loss: 0.6424\n",
      "Accuracy of the network on the 10000 test images: 85.91 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_nn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_nn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model_nn.state_dict(), 'nn_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, 85.91% is a pretty good result for a relatively simple neural network and only 5 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model (after saving it)\n",
    "#model_nn.load_state_dict(torch.load('nn_model.ckpt'))\n",
    "#model_nn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of neural network that finds the function mapping the features x to itself. This objective is known as reconstruction, and an autoencoder accomplishes this through the following process: (1) an encoder learns the data representation in lower-dimension space, i.e. extracting the most salient features of the data, and (2) a decoder learns to reconstruct the original data based on the learned representation by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```encoding_dim``` is the size of the features at the bottleneck of the network. These features are a lot easier to handle and could be potentially used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## very simple autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        # linear layer (784 -> encoding_dim)\n",
    "        self.fc1 = nn.Linear(28 * 28, encoding_dim)\n",
    "        \n",
    "        ## decoder ##\n",
    "        # linear layer (encoding_dim -> input size)\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # output layer (sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "autoencoder = Autoencoder(encoding_dim)\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we train an autoencoder? How do we know what kind of \"encoder\" and \"decoder\" we want?\n",
    "\n",
    "One observation is that if we pass an image through the encoder, then pass the result through the decoder, we should get roughly the same image back. Ideally, reducing the dimensionality and then generating the image should give us the same result.\n",
    "\n",
    "This observation provides us a training strategy: we will minimize the reconstruction error of the autoencoder across our training data. We use a loss function called ```MSELoss```, which computes the square error at every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 5.619568\n",
      "Epoch: 2 \tTraining Loss: 2.895644\n",
      "Epoch: 3 \tTraining Loss: 2.301428\n",
      "Epoch: 4 \tTraining Loss: 1.919472\n",
      "Epoch: 5 \tTraining Loss: 1.737497\n",
      "Epoch: 6 \tTraining Loss: 1.669788\n",
      "Epoch: 7 \tTraining Loss: 1.638515\n",
      "Epoch: 8 \tTraining Loss: 1.586030\n",
      "Epoch: 9 \tTraining Loss: 1.531939\n",
      "Epoch: 10 \tTraining Loss: 1.521207\n",
      "Epoch: 11 \tTraining Loss: 1.513681\n",
      "Epoch: 12 \tTraining Loss: 1.508683\n",
      "Epoch: 13 \tTraining Loss: 1.504673\n",
      "Epoch: 14 \tTraining Loss: 1.501115\n",
      "Epoch: 15 \tTraining Loss: 1.498268\n",
      "Epoch: 16 \tTraining Loss: 1.495739\n",
      "Epoch: 17 \tTraining Loss: 1.493423\n",
      "Epoch: 18 \tTraining Loss: 1.491200\n",
      "Epoch: 19 \tTraining Loss: 1.489174\n",
      "Epoch: 20 \tTraining Loss: 1.487997\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = autoencoder(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for plotting some of the test images along with their reconstructions. For the most part these look pretty good except for some blurriness in some parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images_flatten = images.view(images.size(0), -1)\n",
    "# get sample outputs\n",
    "output = autoencoder(images_flatten)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# output is resized into a batch of images\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRA: Could you create an autoencoder with a more complex architecture (add additional layers)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular neural networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.\n",
    "\n",
    "Regular neural nets don’t scale well to full images. In MNIST, images are only of size 28*28 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28*28 = 784 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3 (3 colour channels), would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! This full connectivity is difficult for imagery and the huge number of parameters would quickly lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at convolutions and CNN-specific layers next week. For this example, we'll use two convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), # Convolutional layer\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.5370\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3088\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1727\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0889\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1861\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1217\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0804\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1441\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1277\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0987\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0538\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1307\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0840\n",
      "Epoch [3/5], Step [200/600], Loss: 0.3337\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0817\n",
      "Epoch [3/5], Step [400/600], Loss: 0.2264\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0782\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1604\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0415\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0905\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0706\n",
      "Epoch [4/5], Step [400/600], Loss: 0.1633\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1416\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0878\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0235\n",
      "Epoch [5/5], Step [200/600], Loss: 0.1112\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0874\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0238\n",
      "Epoch [5/5], Step [500/600], Loss: 0.2761\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0609\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.3 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = cnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(cnn.state_dict(), 'cnn_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The magic of convolutional layers: 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load model (after saving it)\n",
    "cnn.load_state_dict(torch.load('cnn_model.ckpt'))\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about CNNs next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and additional resources:\n",
    "\n",
    " - THE deep learning book: https://www.deeplearningbook.org/\n",
    " - Pytorch: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    " - Autoencoder: https://www.jeremyjordan.me/autoencoders/\n",
    " - Neural network playground (try different hyperparameters and see what happens): https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises\n",
    " - CNNs: https://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
