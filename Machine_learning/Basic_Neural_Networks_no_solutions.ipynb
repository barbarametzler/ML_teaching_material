{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Introduction\n",
    "\n",
    "Neural networks are often applied to complex problems like image recognition, natural language processing, and more. However, they can also be used for the standard classification and regression tasks that you are used to.\n",
    "\n",
    "This week we will be creating a classification model on the PIMA diabetes data - which you have already done in week 5. You will also be creating a regression model in the exercise.\n",
    "\n",
    "If you find the code hard to follow, please check out the NN_Tensor_Basics.ipynb notebook which introduces nerual networks and pytorch at a slower pace.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "pima = pd.read_csv('diabetes.csv')\n",
    "pima.info()\n",
    "\n",
    "y = pima['Outcome']\n",
    "# We want to leave this as 0,1 values of int64 data type\n",
    "pima.drop('Outcome', axis = 1, inplace = True)\n",
    "X = pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale your Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global parameters of your model\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = torch.tensor(np.array(X_data), dtype = torch.float32)\n",
    "        # Note that using float32 means we don't need to change the .parameters() datatype\n",
    "        self.y_data = torch.tensor(np.array(y_data), dtype = torch.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # get and set are common functions to have in a class\n",
    "        # get returns the values that you store, set changes them\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = trainData(X_train, y_train)\n",
    "\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = torch.tensor(np.array(X_data), dtype = torch.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "test_data = testData(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loader \n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 8.\n",
    "        self.layer_1 = nn.Linear(8, 64)\n",
    "        # Number of outputs in one layer must == inputs in the next layer\n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # The activation function that we are going to use\n",
    "        # Can put it here or call it with F.ReLU() in the forward function\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        # This is the dropout rate\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        # Performs mini-batch normalization \n",
    "        ## See additional resources at the bottom if curious\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # this is our forward propagation\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        # We will apply our sigmoid transformation later\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = binaryClassification()\n",
    "# Creating an instance of our model\n",
    "\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Binary Cross Entropy Loss with LogitsLoss\n",
    "# This contains sigmoid in it already so we don't need to include it in our forward layer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    # Classifying how accurate our model is\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    # This is where we turn our output into a 0/1 prediction\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.63094 | Acc: 65.333\n",
      "Epoch 002: | Loss: 0.51134 | Acc: 77.667\n",
      "Epoch 003: | Loss: 0.51412 | Acc: 78.111\n",
      "Epoch 004: | Loss: 0.54557 | Acc: 73.778\n",
      "Epoch 005: | Loss: 0.45924 | Acc: 78.556\n",
      "Epoch 006: | Loss: 0.51186 | Acc: 75.556\n",
      "Epoch 007: | Loss: 0.47909 | Acc: 77.111\n",
      "Epoch 008: | Loss: 0.43489 | Acc: 82.333\n",
      "Epoch 009: | Loss: 0.47919 | Acc: 76.667\n",
      "Epoch 010: | Loss: 0.46396 | Acc: 77.333\n",
      "Epoch 011: | Loss: 0.42739 | Acc: 84.000\n",
      "Epoch 012: | Loss: 0.40304 | Acc: 84.222\n",
      "Epoch 013: | Loss: 0.43886 | Acc: 77.889\n",
      "Epoch 014: | Loss: 0.43034 | Acc: 79.889\n",
      "Epoch 015: | Loss: 0.44224 | Acc: 79.222\n",
      "Epoch 016: | Loss: 0.40896 | Acc: 85.000\n",
      "Epoch 017: | Loss: 0.38559 | Acc: 83.778\n",
      "Epoch 018: | Loss: 0.44857 | Acc: 74.889\n",
      "Epoch 019: | Loss: 0.36999 | Acc: 85.667\n",
      "Epoch 020: | Loss: 0.37189 | Acc: 85.000\n",
      "Epoch 021: | Loss: 0.35637 | Acc: 86.000\n",
      "Epoch 022: | Loss: 0.33963 | Acc: 87.000\n",
      "Epoch 023: | Loss: 0.37231 | Acc: 86.333\n",
      "Epoch 024: | Loss: 0.45434 | Acc: 82.667\n",
      "Epoch 025: | Loss: 0.32658 | Acc: 87.222\n",
      "Epoch 026: | Loss: 0.47447 | Acc: 75.556\n",
      "Epoch 027: | Loss: 0.36079 | Acc: 87.333\n",
      "Epoch 028: | Loss: 0.33346 | Acc: 87.333\n",
      "Epoch 029: | Loss: 0.40456 | Acc: 80.778\n",
      "Epoch 030: | Loss: 0.30063 | Acc: 88.000\n",
      "Epoch 031: | Loss: 0.36561 | Acc: 82.778\n",
      "Epoch 032: | Loss: 0.32580 | Acc: 88.667\n",
      "Epoch 033: | Loss: 0.31781 | Acc: 87.000\n",
      "Epoch 034: | Loss: 0.32374 | Acc: 88.000\n",
      "Epoch 035: | Loss: 0.30908 | Acc: 88.444\n",
      "Epoch 036: | Loss: 0.29341 | Acc: 90.667\n",
      "Epoch 037: | Loss: 0.28937 | Acc: 88.222\n",
      "Epoch 038: | Loss: 0.32902 | Acc: 84.556\n",
      "Epoch 039: | Loss: 0.34485 | Acc: 82.556\n",
      "Epoch 040: | Loss: 0.38593 | Acc: 82.556\n",
      "Epoch 041: | Loss: 0.39814 | Acc: 84.111\n",
      "Epoch 042: | Loss: 0.35226 | Acc: 83.778\n",
      "Epoch 043: | Loss: 0.31865 | Acc: 84.333\n",
      "Epoch 044: | Loss: 0.41931 | Acc: 78.778\n",
      "Epoch 045: | Loss: 0.44499 | Acc: 83.111\n",
      "Epoch 046: | Loss: 0.35509 | Acc: 83.889\n",
      "Epoch 047: | Loss: 0.34048 | Acc: 84.667\n",
      "Epoch 048: | Loss: 0.29346 | Acc: 90.333\n",
      "Epoch 049: | Loss: 0.32947 | Acc: 84.222\n",
      "Epoch 050: | Loss: 0.36612 | Acc: 83.556\n"
     ]
    }
   ],
   "source": [
    "## Training our model\n",
    "\n",
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    # Iterate through the epochs\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Iterate through the batches our dataloader created\n",
    "        optimizer.zero_grad()\n",
    "        # This removes the stored gradient\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        # From forward propagation\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        # Calculate the loss\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        # Calculate accuracy\n",
    "        \n",
    "        loss.backward()\n",
    "        # Perform backwards propagation\n",
    "        optimizer.step()\n",
    "        # Update the weights\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[139,  25],\n",
       "       [ 41,  49]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       164\n",
      "           1       0.66      0.54      0.60        90\n",
      "\n",
      "    accuracy                           0.74       254\n",
      "   macro avg       0.72      0.70      0.70       254\n",
      "weighted avg       0.73      0.74      0.73       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "So we got an accuracy of .74, a recall of .56 and a precision of .65. This is a kaggle competition and the general consensus is that [78 % is very good](https://www.kaggle.com/general/19387), there are some scores in the low 80s, while the best (unverified) results was 90.6%!\n",
    "\n",
    "So our results are very good - but not the best it could be. We haven't tuned our model at all and it isn't a very deep model either - with the just one hidden layer.\n",
    "\n",
    "What would you do to improve the results? The 'Basics' notebook has some suggestions. We won't try to get better results here as this is just an introduction to the structure of a network, but it is worth thinking about. I played around with the number of epochs to see if I could get an easy boost and actually got a worse accuracy with 500 epochs - meaning that I overfit and should either increase the drop out rate or leave epochs at 50 and tune other parameters.\n",
    "\n",
    "# Your Turn\n",
    "\n",
    "We just made a classification model that was a more complex version of a logistic regression. Your task is to make a network that is more complex version of a linear regression to predict the BMI variable in the PIMA dataset. The data has not been collected with BMI prediction in mind - so we shouldn't expect good results. But it is a great exercise to see if you can adapt the code above to generate a numeric prediction!\n",
    "\n",
    "You should\n",
    "- have at least 3 layers\n",
    "- use the train_data and test_data classes from above (no need to rewrite them)\n",
    "- print the mean squared error and loss for each epoch. Printing out the MSE could be a little tricky so start out with just the loss.\n",
    "- print the root mean squared error (so it is on the same scale as bmi), mean absolute error and r2 for the test set\n",
    "- feel free to change parameters (you will need to change at least one) \n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "pima = pd.read_csv('diabetes.csv')\n",
    "pima.info()\n",
    "\n",
    "y = pima['BMI']\n",
    "pima.drop('BMI', axis = 1, inplace = True)\n",
    "X = pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our new network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did we do?\n",
    "\n",
    "No very good - as expected. Our RMSE and MAE suggest that we are 12-14 BMI points away from the truth on average. 'Healthy' BMI is 18-25 (7 BMI points) while overweight is 25-30 (5 BMI points) - so being 12-14 off on average will predict people in the wrong category a majority of the time. Our r2 score is also negative - meaning it predicts worse than a horizontal line.\n",
    "\n",
    "Some tuning probably won't help us out  here - as the low r2 indicates that perhaps the data is just not good enough to generate a strong prediction. This would be an indication to abandon this method and to try out some classical methods to determine whether the method doesn't fit the data or whether the data is just substandard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "[Batch Normalization](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)\n",
    "\n",
    "[Data Loaders](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
