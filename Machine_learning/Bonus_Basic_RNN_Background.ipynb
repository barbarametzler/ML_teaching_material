{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is optional content meant to tackle the basics of RNNs. If you find yourself not understanding the lecture or if the practical is too confusing, please read this document to see if it helps your understand. As it is optional content, there will not be a video on this - but feel free to post questions on the discussion board. Even though the course is 'over' we will look for questions for a few more weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "Recurrent Neural Networks model sequential data. They use information from the previous data points to inform decisions made with the current observation. By using time-series information to identify patterns between the input and output, the memory of RNN algorithms allows them to learn more about long-term dependencies in data and understand the whole context of the sequence while making the next prediction. It introduces sparsity and reuses the same neurons and weights over time. RNNs have a lot of applications, with sequential data making up audio, text, video and more. DNA sequencing is also an exciting potential application for RNNs.\n",
    "\n",
    "RNNs are most commonly applied to text data - where the words are transformed into machine-readable vectors and processed one by one. \n",
    "\n",
    "![RNN Diagram](https://miro.medium.com/max/700/1*AQ52bwW55GsJt6HTxPDuMA.gif)\n",
    "\n",
    "RNNs are different from CNNs as they work on temporal instead of spatial data - neither is better than the other, just suited for different tasks. RNNs can also be used together with CNNs - where are known as CRNNs. They have applications in gesture recognition, video scene labelling, video identification, and DNA sequencing. CRNNs will not be covered in this course.\n",
    "\n",
    "### Sequential Memory\n",
    "\n",
    "How does an RNN take into account the previous observations? Through using a hidden state to pass information backwards in the model. This looping structure is a departure from the feed forward networks that you have seen - where information only flows in one direction.\n",
    "\n",
    "![Hidden State](https://miro.medium.com/max/250/1*T_ECcHZWpjn0Ki4_4BEzow.gif)\n",
    "\n",
    "In the picture below, you can see the information from hidden states being used in following calculations. \n",
    "\n",
    "![Hidden State Example](https://miro.medium.com/max/500/1*d_POV7c8fzHbKuTgJzCxtA.gif)\n",
    "\n",
    "The input and previous hidden state are combined to form a vector which undergoes a transformation (from using one of our activation functions) and becomes the new hidden state.\n",
    "\n",
    "![Detailed Hidden State](https://miro.medium.com/max/700/1*WMnFSJHzOloFlJHU6fVN-g.gif)\n",
    "\n",
    "You can pass the last output of an RNN into a feed forward layer to return a final prediction. The control flow of doing a forward pass of a recurrent neural network is often a for loop.\n",
    "\n",
    "## Vanishing Gradient\n",
    "\n",
    "In the picture above, the color representation of each previous observatioin in the hidden state gets smaller as the sequence increases. This short-term memory is caused by a vanishing gradient due to the back propagation step. \n",
    "\n",
    "When calculating the gradient of your backpropagation, you chain together each loss function and calculate each of the individual layer's gradient in respect to the gradients of the layers before it (before in the back propagation - meaning further along in the network architecture). If a change to one of the prior gradients is small, the next change will be even smaller. This relationship is exponential - causing earlier layers to barely be adjusted if the gradient of the last layer is small. In other words, the gradient will vanish over the course of the back propagation as it gets exponentially smaller.\n",
    "\n",
    "## Back Propagation through time\n",
    "\n",
    "Below is an extremly simplied version of a RNN. You can see that each 'layer' is a step in time - the input is time 0 and word 0, the next layer is time 1 and word 1, and so on (indexing at 0). As such, RNNs use back propagation through time - where gradients are used to adjust the neural network weights. \n",
    "\n",
    "![Simple Back Prop through time](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/12/07121550/rnn-intermediate.png)\n",
    "\n",
    "This means that if the first gradients that are computed are small, then we will again run into the vanishing gradient problem. In recurrent neural networks, layers that get a small gradient update stop learning. This means that the RNN might not consider the earlier observations in the hidden state. In our example of 'what time is it?', the network may only consider 'is it?' This is an example of short term memory and can hamper the performance of your network.\n",
    "\n",
    "## Solving short term memory\n",
    "\n",
    "Luckily, there are RNN architectures designed to combat this issue. LSTM (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are the most popular. These designs rely on 'gates' or tensor operations that learn what information to add or remove from the hidden states - or which data is important to keep or can be thrown away. In our example above, perhaps only 'time is it?' is needed to generate a good response. You should use LSTM’s or GRU’s when you expect to model longer sequences with long-term dependencies. They are used for most state of the art RNN models that perform tasks like speech recognition, speech synthesis, text generation, and caption generation. \n",
    "\n",
    "## LTSM\n",
    "\n",
    "LTSMs processes data by passing on information as it does its forward pass. It uses gates within its 'cell state' to add or remove data from the hidden state in order to pass information down the sequential chain. The gates learn what actions to take over the course of training.\n",
    "\n",
    "The gates contain sigmoid transformations. This allows data to be forced to be a 0. This can be used to forget information (since 0 * info = 0). There are three gates in the structure: the forget gate, input gate, and output gate.\n",
    "\n",
    "![LTSM](https://miro.medium.com/max/700/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
    "\n",
    "The forget gate is the first combines information from the previous hidden state with the current input before passing it through the sigmoid function - resulting in a value between 0 and 1. \n",
    "\n",
    "![Forget gate](https://miro.medium.com/max/700/1*GjehOa513_BgpDDP6Vkw2Q.gif)\n",
    "\n",
    "The input gate combines the previous hidden state and the current input - passing them through before a sigmoid transformation and a tanh activation. These two values are then multiplied together, allowing the sigmoid to determine what parts of the tanh output is important to keep.\n",
    "\n",
    "![Input gate](https://miro.medium.com/max/700/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)\n",
    "\n",
    "The cell state is calculated next - which is essentially the output of this layer (or time point). The previous cell state is multiplied by the output of the forget gate - determing which parts to forget and which to remember. Next, this value is added to the output from the input gate - generating the final cell state.\n",
    "\n",
    "![Cell State](https://miro.medium.com/max/700/1*S0rXIeO_VoUVOyrYHckUWg.gif)\n",
    "\n",
    "The output gate determines the next hidden state. The previous hidden state is combined with the current input again - once again being passed into a sigmoid transformation. This value is multiplied by the cell state (after it has been through a sigmoid transformation) - generating the hidden state for the next layer.\n",
    "\n",
    "![Output Gate](https://miro.medium.com/max/700/1*VOXRGhOShoWWks6ouoDN3Q.gif)\n",
    "\n",
    "\n",
    "## GRU\n",
    "\n",
    "GRUs do not use the cell state and only have two gates - the reset gate and update gate.\n",
    "\n",
    "![GRU](https://miro.medium.com/max/700/1*jhi5uOm9PvZfmxvfaCektw.png)\n",
    "\n",
    "The Update gate is similar to the forget and input gates of the LTSM - it determines which information to add and which to not incorporate.\n",
    "\n",
    "The reset gate determines how much past information should be forgetting.\n",
    "\n",
    "GRUs have a simplier architecture, so are typically faster than LTSMs. You should test both out to see which fits your problem the best.\n",
    "\n",
    "## Types of RNNs\n",
    "\n",
    "Most neural network frameworks only allow for one vector of inputs and one vector of outputs. RNNs are different in that they can have multiple vectors of outputs or inputs. In the picture below, inputs are the red boxes while outputs are the blue boxes.\n",
    "\n",
    "![Types of RNNs](https://miro.medium.com/max/700/0*1PKOwfxLIg_64TAO.jpeg)\n",
    "\n",
    "One to one RNNs are in the familiar format that you have seen before. They are often used for image classification.\n",
    "\n",
    "One to many RNNs has a fixed size of input, but the outputs can vary in size. Image captioning takes an image (of fixed size) as input and outputs a sequence of words. Each image can have a different number of words outputted.\n",
    "\n",
    "Many to one RNNs take a varied input size and generates a fixed output. In sentiment analysis text of varying lengths are feed into the network to find a fixed size description (often somthing like 'angry').\n",
    "\n",
    "In many to many RNNs, both inputs and outputs can vary in size. Video classification problems have videos of varying lengths and generate labels of different lengths as outputs.\n",
    "\n",
    "## Pytorch and RNNs\n",
    "\n",
    "Below is a simple example of using an RNN to generate text. This is not a final solution that you would want to implement, but does show you all of the new parts of the architecture without getting bogged down with a lot of preprocessing (which using text requires). If you want to see a great tutorial for classifying the nationality of last names, check [this](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) out. A warning, there is a lot of code before you get to building a network.\n",
    "\n",
    "On to the example. The goal is to give our model a word or a few letters and have it generate a sentence. We need to give it some sentences and map those character values to integers. You would normally use a text file (a poem, review, etc.), but we are going to manually enter just three sentences to focus on the network element of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'h', 1: 'r', 2: 'i', 3: 'u', 4: 'd', 5: 'c', 6: 'm', 7: ' ', 8: 'o', 9: 'y', 10: 'v', 11: 'g', 12: 'n', 13: 'a', 14: 'e', 15: 'w', 16: 'f'}\n",
      "{'h': 0, 'r': 1, 'i': 2, 'u': 3, 'd': 4, 'c': 5, 'm': 6, ' ': 7, 'o': 8, 'y': 9, 'v': 10, 'g': 11, 'n': 12, 'a': 13, 'e': 14, 'w': 15, 'f': 16}\n"
     ]
    }
   ],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join sentences together \n",
    "# extract the unique characters\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Create a dictionary of integer values for the characters\n",
    "# note that the integer value does not correspond to its position in the alphabet\n",
    "# you don't need to use the same value for a character in each model you build\n",
    "# just define it and be consistent in the model you are working on\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "print(int2char)\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all of the sentences to be the same length - a one to one RNN. While we could make a many to one (allowing for varying lengths), using batch training is easier to perform when all of the inputs have the same size. To do this, we will add blank spaces to the shorter sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earliier, we are going to predict the next character in a sequence. So if we give it 'G', we want it to predict 'o' (as the word is good). Because of this, we are going to exclude the last character from the input - since there is nothing after it to predict. We will also exclude the first character from the output (or our target) - since there is nothing before it to spark the prediction. \n",
    "\n",
    "After this, we will use our dictionary from above to turn out characters into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "# Creating inputs and outputs\n",
    "\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    \n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 14, 9, 7, 0, 8, 15, 7, 13, 1, 14, 7, 9, 8], [11, 8, 8, 4, 7, 2, 7, 13, 6, 7, 16, 2, 12, 14], [0, 13, 10, 14, 7, 13, 7, 12, 2, 5, 14, 7, 4, 13]]\n",
      "[[14, 9, 7, 0, 8, 15, 7, 13, 1, 14, 7, 9, 8, 3], [8, 8, 4, 7, 2, 7, 13, 6, 7, 16, 2, 12, 14, 7], [13, 10, 14, 7, 13, 7, 12, 2, 5, 14, 7, 4, 13, 9]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "    \n",
    "print(input_seq)\n",
    "print(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to change the numeric values above to be a one hot encoded array. We need to have a column for each of the unique characters in our input data with a 1 in the column corresponding to the character and 0 everywhere else. We have this information in the length of our dictionaries. We will use the length of the sequence (or sentence) - that we made into a standard size - to iterate over each of the cells in this one hot encoded vector. Lastly, we need to make sure that we are doing this for each sentence. We perform all of these tasks in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "print(input_seq[0]) # the first sentence\n",
    "\n",
    "# ! Note - if you run this twice you will get an error\n",
    "# This is because you are changing what input_seq is and it is an input to the function\n",
    "# run the cell 'Creating inputs and outputs' that is three cells above this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)\n",
    "\n",
    "# Select your device\n",
    "## GPUs are faster than cpus - but macs don't have them\n",
    "# is_cuda = torch.cuda.is_available()\n",
    "## That line will tell you if you have a gpu, if you do - run\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our RNN\n",
    "\n",
    "We are finally ready to creat our network. We will be using the class structure again so that we can inherit functionality from nn.Module. This simple model will have one RNN layer and one fully connected layer. As mentioned earlier, the feed forward layer (fully-connected) will convert our RNN output into a prediction. \n",
    "\n",
    "Remember that we need to have a hidden state that has the sequential memory. To do this, we define a function called 'init_hidden' that makes the first hidden state (all 0s). We then get a new hidden value out after we run our RNN. Some people use for loops to add more RNN layers using the same format as below (for i in range(5): out, hidden = self.rnn(x, hidden) since this updates your hidden and out values each time. \n",
    "\n",
    "We do not hard code the input size and output size. We will be supplying these when creating an instance of our class later. Since each of our characters is a one hot encoded vector whose length is the number of unique characters in all of our text, we will use the length of one of the dictionaries for both of these values.\n",
    "\n",
    "n_layer corresponds to how many RNN layers we will be passing our data through. We will set this to be one here but you could change this to higher number if you want. \n",
    "\n",
    "Lastly, we have hidden_dim which defines the size of the hidden state. We will make this 12 - as it does not need information about the first or last elements of the sequence (and each sequence is 14 characters long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #first hidden state - all 0s\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # rnn layer\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Model(input_size=dict_size, \n",
    "              output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Hyperparameters - feel free to change these and see what happens\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # since essentially a classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.3949\n",
      "Epoch: 20/100............. Loss: 2.1411\n",
      "Epoch: 30/100............. Loss: 1.7562\n",
      "Epoch: 40/100............. Loss: 1.3119\n",
      "Epoch: 50/100............. Loss: 0.9454\n",
      "Epoch: 60/100............. Loss: 0.6671\n",
      "Epoch: 70/100............. Loss: 0.4769\n",
      "Epoch: 80/100............. Loss: 0.3510\n",
      "Epoch: 90/100............. Loss: 0.2611\n",
      "Epoch: 100/100............. Loss: 0.1950\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Calculates gradients\n",
    "    optimizer.step() # Updates the weights \n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained our model. While that was lightening fast - we only had three sentences. As always, training your neural network can take a really long time.\n",
    "\n",
    "So we want to be able to give the model a letter and have it predict the next one. We want this action to be repeated until we have iterated over all of the characters in the input sentence. In a normal problem, you wouldn't use the same sentences for inputs and outputs since you would have a lot of text data - but we will make do with what we have. \n",
    "\n",
    "We first create a function called 'predict'. It takes in a model (our trained model) and a single charcter. It converts the character into an integer, then a one hot encoded array, and then a tensor. After it is feed into the model, we use softmax to determine which value is most likely. Finally, we convert the index of the highest predicted value back into a character and return it.\n",
    "\n",
    "The function 'sample' does our prediction. It tells the model to evaluate. It then takes the word we are starting with and turns it into a vector of characters. Each of those characters is passed inito the 'predict' function that returns our predicted character. These are joined together and we get our final sentence. Let's see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good i am fine \n",
      "hey how are you\n",
      "have a nice day\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 15, 'good'))\n",
    "print(sample(model, 15, 'hey'))\n",
    "print(sample(model, 15, 'have'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the end of the notebook! I hope that you enjoyed it. Feel free to create your own sentences and see if you can generate the right output. When I changed the text to ['I love machine learning','Dragana isreally cool','have lots of fun studying'] I could get 2 out of the 3 predicted correctly. Can you find out which one didn't work? Note: you have to change more than just the text = ... part.\n",
    "\n",
    "Below are a bunch of additional resources. As always, you don't have to read them all but if you are still confused check out the links to see if coming at it from a different angle is more helpful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "#### RNN\n",
    "\n",
    "[RNN 'Cheat Sheet'](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "\n",
    "[RNN code essentially from scratch - no advanced packages](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "\n",
    "[Wikipedia on RNN - 19 architectures to look at](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "\n",
    "[Detailed RNN/LTSM explaination with lots of pictures](https://builtin.com/data-science/recurrent-neural-networks-and-lstm)\n",
    "\n",
    "[MNIST with RNN](https://www.dezyre.com/recipes/run-basic-rnn-model-using-pytorch)\n",
    "\n",
    "[Advanced types of RNNs with code and pictures](https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677)\n",
    "\n",
    "\n",
    "#### LSTM\n",
    "\n",
    "[Detailed LSTM explaination with light math](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "[Detailed description the data flow of the LSTM architecture](https://towardsdatascience.com/lstms-in-pytorch-528b0440244)\n",
    "\n",
    "[LSTM Sentiment Analysis with Pytorch](https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948)\n",
    "\n",
    "[Youtube video on LTSM - 26 mins](https://www.youtube.com/watch?v=WCUNPb-5EYI)\n",
    "\n",
    "[Incredibly detailed - code (keras), pictures and math](https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn)\n",
    "\n",
    "[LSTM Time Series in Keras](https://stackabuse.com/time-series-analysis-with-lstm-using-pythons-keras-library/)\n",
    "\n",
    "[GRU/LSTM Implementation with code and math](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n",
    "\n",
    "#### GRU\n",
    "\n",
    "[Detailed GRU - pictures and code](https://blog.floydhub.com/gru-with-pytorch/)\n",
    "\n",
    "[Pytorch GRU documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
    "\n",
    "[Pytorch GRU tutorial Video - not official](https://morioh.com/p/8cbd4d8604eb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
