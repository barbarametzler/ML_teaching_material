{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTSM Using TensorFlow and Keras\n",
    "\n",
    "This is an optional notebook that shows you how to do sentiment analysis and introduces the tensorflow framework. There will be no video for this notebook as it is not the main notebook for this week. As always, feel free to post any questions to the discussion board. \n",
    "\n",
    "#### Sentiment analysis on movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will solve a sequence classification problem by building a RNN with LTSM using Keras and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the IMDB dataset in Keras which contains a set of movie reviews with their associated sentiment. Sentiments could be either positive or negative. Thus, the problem we are solving is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB dataset consists of 25000 movie reviews for training and 25000 for testing. The problem is to predict whether a review has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build our model, we need to import the necessary libraries to perform our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#import numpy and matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras contains a version of the IMDB dataset which we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words=10000\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data(num_words=num_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird review huh? The words are encoded as a sequence of word indexes where words are indexed by overall frequency. For example, \"3\" encodes the 3rd most frequent word in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now decode the training data using the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will alter the input sequences so that they all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider the first 400 words within the review\n",
    "max_review_length = 400\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our model. \n",
    "\n",
    "We will have sequential layers with an input layer, an LTSM layer, and dense output layer. The input layer is of size 32 for each input word. The second layer is an LTSM layer with size 100, and finally one output node since we have a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 328,353\n",
      "Trainable params: 328,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 0.4298 - accuracy: 0.7937 - val_loss: 0.3188 - val_accuracy: 0.8731\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 84s 216ms/step - loss: 0.2386 - accuracy: 0.9083 - val_loss: 0.3104 - val_accuracy: 0.8741\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 84s 216ms/step - loss: 0.1871 - accuracy: 0.9322 - val_loss: 0.3110 - val_accuracy: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2e8e4b710>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "embedding_vecor_length = 32\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, lets look at the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.05%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!\n",
    "\n",
    "In this lab, we learned how to load data from keras datasets, pad and trim data to a certain size, build an LTSM model, and evaluation test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, there are also a bunch of packages that can be used to simplify the NLP process. Check out the last additional resource link if you want to see an code example using the NLTK package.\n",
    "\n",
    "## Packages\n",
    "\n",
    "Note: The following has been combined from several websites. It states things like 'our favourite package' despite the fact that I am not familiar with any of them. \n",
    "\n",
    "NLTK - natural language toolkit\n",
    "\n",
    "Does classification, stemming, tagging, parsing, semantic reasoning, and tokenization in Python. Main tool for NLP. Good for beginners. Can be very slow however. Its modularized structure makes it excellent for learning and exploring NLP concepts, but it’s not meant for production.\n",
    "\n",
    "TextBlob\n",
    "\n",
    "A must for people's first encounter with NLTK - whatever that means. An easy interface to help learn most basic NLP tasks like sentiment analysis, pos-tagging, or noun phrase extraction. Great for beginners, still very slow. This is our favorite library for fast-prototyping or building applications that don’t require highly optimized performance. Beginners should start here.\n",
    "\n",
    "CoreNLP\n",
    "Really fast and works in product development enviornments. Can be integrated with NLTK to boost performance. Written in Java with Python wrappers.\n",
    "\n",
    "Gensim\n",
    "Gensim is a Python library that specializes in identifying semantic similarity between two documents through vector space modeling and topic modeling toolkit. It can handle large text corpora with the help of efficiency data streaming and incremental algorithms, which is more than we can say about other packages that only target batch and in-memory processing. What we love about it is its incredible memory usage optimization and processing speed. These were achieved with the help of another Python library, NumPy. The tool’s vector space modeling capabilities are also top notch.\n",
    "\n",
    "spaCy\n",
    "spaCy is a relatively young library was designed for production usage. That’s why it’s so much more accessible than other Python NLP libraries like NLTK. spaCy offers the fastest syntactic parser available on the market today. Moreover, since the toolkit is written in Cython, it’s also really speedy and efficient. SpaCy is a new NLP library that’s designed to be fast, streamlined, and production-ready. It’s not as widely adopted, but if you’re building a new application, you should give it a try.\n",
    "\n",
    "However, no tool is perfect. In comparison to the libraries we covered so far, spaCy supports the smallest number of languages (seven). However, the growing popularity of machine learning, NLP, and spaCy as a key library means that the tool might start supporting more programming languages soon\n",
    "\n",
    "polyglot\n",
    "This slightly lesser-known library is one of our favorites because it offers a broad range of analysis and impressive language coverage. Thanks to NumPy, it also works really fast. Using polyglot is similar to spaCy – it’s very efficient, straightforward, and basically an excellent choice for projects involving a language spaCy doesn’t support. The library stands out from the crowd also because it requests the usage of a dedicated command in the command line through the pipeline mechanisms. Definitely worth a try\n",
    "\n",
    "scikit-learn\n",
    "This handy NLP library provides developers with a wide range of algorithms for building machine learning models. It offers many functions for using the bag-of-words method of creating features to tackle text classification problems. The strength of this library is the intuitive classes methods. Also, scikit-learn has an excellent documentation that helps developers make the most of its features.\n",
    "\n",
    "However, the library doesn’t use neural networks for text preprocessing. So if you’d like to carry out more complex preprocessing tasks like POS tagging for your text corpora, it’s better to use other NLP libraries and then return to scikit-learn for building your models\n",
    "\n",
    "Pattern\n",
    "Another gem in the NLP libraries Python developers use to handle natural languages. Pattern allows part-of-speech tagging, sentiment analysis, vector space modeling, SVM, clustering, n-gram search, and WordNet. You can take advantage of a DOM parser, a web crawler, as well as some useful APIs like Twitter or Facebook. Still, the tool is essentially a web miner and might not be enough for completing other natural language processing tasks\n",
    "\n",
    "CoreNLP is a Java library with Python wrappers. It’s in many existing production systems due to its speed.\n",
    "\n",
    "\n",
    "Gensim is most commonly used for topic modeling and similarity detection. It’s not a general-purpose NLP library, but for the tasks it does handle, it does them well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "#### NLP\n",
    "\n",
    "[8 best NLP Libraries](https://sunscrapers.com/blog/8-best-python-natural-language-processing-nlp-libraries/)\n",
    "\n",
    "[Basic NLP explainations](https://towardsdatascience.com/gentle-start-to-natural-language-processing-using-python-6e46c07addf3)\n",
    "\n",
    "[Definitions of NLP terms](https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_python.htm)\n",
    "\n",
    "[Great Short Datacamp Tutorial on Sentiment Analysis and Text Classification - NLTK package](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
